{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "forecast_heart_signal.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "mount_file_id": "1G8SEGdo2Rb19jrkyvDvgYryklyity0Zm",
   "authorship_tag": "ABX9TyNXfoSHLeT7vS7N2AFoo6qI"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "天池心跳信号预测\n",
    "https://tianchi.aliyun.com/competition/entrance/531883/information"
   ],
   "metadata": {
    "id": "sqgopgZHbXM_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdHhRY1aWkax"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data as Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "设置参数"
   ],
   "metadata": {
    "id": "dmgw5_mfbbwq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 256\n",
    "train_split = 0.9  # 训练集比例\n",
    "valid_split = 0.1  # 测试机比例\n",
    "epoch = 130"
   ],
   "metadata": {
    "id": "Dl-FYDskbdWE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "读取数据集"
   ],
   "metadata": {
    "id": "GwEK9tdMbfSZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 定义数据适配器\n",
    "class DataAdapter(Data.Dataset):\n",
    "\n",
    "    def __init__(self, X, Y):\n",
    "        super(DataAdapter, self).__init__()\n",
    "        self.X = torch.FloatTensor(X)  # 特征值为float类型\n",
    "        self.Y = torch.LongTensor(Y)  # 标签值为int类型\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index, :], self.Y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "\n",
    "def read_data(batch_size, train_split, valid_split):\n",
    "    signal = []\n",
    "    label = []\n",
    "\n",
    "    train_data = r'/content/drive/MyDrive/Colab/DeepLearning/心跳信号预测/train.csv'  # 训练文件路径\n",
    "    # 用pandas读取数据，在对特征进行拆分的时候，会由于内存不足报错\n",
    "    with open(train_data, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            signal.append([float(num) for num in line['heartbeat_signals'].split(',')])  # 拆分后的数据\n",
    "            label.append(int(float(line['label'])))  # 标签\n",
    "\n",
    "    dataset = DataAdapter(signal, label)  # 构造数据集\n",
    "    train_size = int(len(signal) * train_split)\n",
    "    valid_size = len(signal) - train_size\n",
    "    train_dataset, valid_dataset = Data.random_split(dataset, [train_size, valid_size])  # 随机划分训练集和验证集\n",
    "\n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)  # 加载DataLoader\n",
    "    valid_loader = Data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "\n",
    "# 定义该函数用于重新打乱训练集和验证集\n",
    "def shuffle_data(train_loader, valid_loader, valid_split, batch_size):\n",
    "    train_dataset = train_loader.dataset.dataset  # 获取训练集的数据集\n",
    "    valid_dataset = valid_loader.dataset.dataset\n",
    "    X = torch.cat((train_dataset.X, valid_dataset.X), 0)  # 拼接数据集\n",
    "    Y = torch.cat((train_dataset.Y, valid_dataset.Y), 0)\n",
    "    dataset = DataAdapter(X, Y)  # 重新生成数据集\n",
    "    train_dataset, valid_dataset = Data.random_split(dataset, [len(dataset) - int(len(dataset) * valid_split), int(len(dataset) * valid_split)])  # 重新划分训练集和验证集\n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    valid_loader = Data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    return train_loader, valid_loader"
   ],
   "metadata": {
    "id": "8VpXHo4hbiBD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练一个epoch"
   ],
   "metadata": {
    "id": "WZ2Pm5TSbuXM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 定义训练函数\n",
    "def train_model(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()  # 获取数据\n",
    "\n",
    "        outputs = model(inputs)  # 预测结果\n",
    "        \n",
    "        _, pred = outputs.max(1)  # 求概率最大值对应的标签\n",
    "\n",
    "        num_correct = (pred == labels).sum().item()\n",
    "        acc = num_correct / len(labels)  # 计算准确率\n",
    "\n",
    "        loss = criterion(outputs, labels)  # 计算loss\n",
    "        optimizer.zero_grad()  # 梯度清0\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新系数\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        train_acc.append(acc)\n",
    "\n",
    "    return np.mean(train_loss), np.mean(train_acc)"
   ],
   "metadata": {
    "id": "JXOz4kgQbx8X"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试函数"
   ],
   "metadata": {
    "id": "Y3GAM3R1b08m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 定义测试函数，具体结构与训练函数相似\n",
    "def test_model(test_loader, criterion, model, device):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    " \n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, pred = outputs.max(1)\n",
    "\n",
    "        num_correct = (pred == labels).sum().item()\n",
    "        acc = num_correct / len(labels)\n",
    "        # 测试不需要反向传播\n",
    "        test_loss.append(loss.item())\n",
    "        test_acc.append(acc)\n",
    "\n",
    "    return np.mean(test_loss), np.mean(test_acc)"
   ],
   "metadata": {
    "id": "sP1ZT_vAb2jA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义模型"
   ],
   "metadata": {
    "id": "X8q0RB0db6jS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 定义模型结构\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv1d(32, 64, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv1d(64, 64, 5, 1, 2)\n",
    "        self.conv5 = nn.Conv1d(64, 128, 5, 1, 2)\n",
    "        self.conv6 = nn.Conv1d(128, 128, 5, 1, 2)\n",
    "        self.maxpool = nn.MaxPool1d(3, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(6400, 256)\n",
    "        self.fc21 = nn.Linear(6400, 16)\n",
    "        self.fc22 = nn.Linear(16, 256)\n",
    "        self.fc3 = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, x.size(1))\n",
    "        x = self.conv1(x)  # nn.Conv1d(in_channels = 1,out_channels = 32,kernel_size = 11,stride = 1,padding = 5)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x1 = self.fc1(x)\n",
    "        x1 = self.relu(x1)\n",
    "        x21 = self.fc21(x)\n",
    "        x22 = self.relu(x21)\n",
    "        x22 = self.fc22(x22)\n",
    "        x2 = self.sigmoid(x22)\n",
    "        x = self.fc3(x1 + x2)\n",
    "        return x"
   ],
   "metadata": {
    "id": "9OXq0CTPb7pk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "预测函数"
   ],
   "metadata": {
    "id": "_l48ChkRb_vc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def predict_ali_testset(batch_size, model, device):\n",
    "    '''\n",
    "    该函数用于生成预测文件\n",
    "    '''\n",
    "    ipath = r'/content/drive/MyDrive/Colab/DeepLearning/心跳信号预测'  # 输入数据文件路径\n",
    "    opath = r'/content/drive/MyDrive/Colab/DeepLearning/心跳信号预测'  # 输出提交文件路径\n",
    "    signal = []\n",
    "\n",
    "    with open(os.path.join(ipath, 'testA.csv'), 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in reader:\n",
    "            signal.append([float(num) for num in line['heartbeat_signals'].split(',')])  # 拆分特征\n",
    "\n",
    "    test_set = DataAdapter(signal, [0 for i in range(len(signal))])\n",
    "    test_loader = Data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "\n",
    "            inputs,_ = data[0].cuda(),data[1].cuda()  # 读取数据\n",
    "            # 预测结果\n",
    "            outputs = model(inputs)\n",
    "            _, pred = outputs.max(1)\n",
    "            # 将预测结果转成numpy\n",
    "            pred_npy = pred.cpu().numpy()\n",
    "            # 转成四元组形式\n",
    "            for ii in range(len(pred_npy)):\n",
    "                a = [0, 0, 0, 0]\n",
    "                a[pred_npy[ii]] = 1\n",
    "                res.append(a)\n",
    "\n",
    "    res = pd.DataFrame(res)\n",
    "\n",
    "    result = pd.read_csv(os.path.join(ipath, 'sample_submit.csv'))  # 构造输出文件\n",
    "    result['label_0'] = res[0]\n",
    "    result['label_1'] = res[1]\n",
    "    result['label_2'] = res[2]\n",
    "    result['label_3'] = res[3]\n",
    "    result['id'] = [i for i in range(100000, 120000)]\n",
    "    result.to_csv('/content/drive/MyDrive/Colab/DeepLearning/心跳信号预测/sample_submit.csv', index=False)\n",
    "\n",
    "    print('预测文件写入完成')"
   ],
   "metadata": {
    "id": "_MCJgboycYut"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "整合代码"
   ],
   "metadata": {
    "id": "NSYwtqfkd1I_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model = CNN().cuda()  # 初始化模型，\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print('******开始读取数据******')\n",
    "\n",
    "    start = time.time()\n",
    "    train_loader, valid_loader = read_data(batch_size, train_split, valid_split)\n",
    "    end = time.time()\n",
    "    run = end - start\n",
    "    print('[1]load： %.5f sec' % run)\n",
    "\n",
    "    print('******开始打乱数据******')\n",
    "\n",
    "    train_loader, valid_loader = shuffle_data(train_loader, valid_loader, valid_split, batch_size)  # 打乱训练集及验证集\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # 使用Adam优化算法\n",
    "    clr = CosineAnnealingLR(optimizer, T_max=150)  # 使用余弦退火算法改变学习率\n",
    "    best_loss = 10\n",
    "\n",
    "    print('******开始训练模型******')\n",
    "    start = time.time()\n",
    "    for epoch in range(epoch):\n",
    "        time_all = 0\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = train_model(train_loader, model, criterion, optimizer, device)  # 训练模型\n",
    "        clr.step()  # 学习率迭代\n",
    "        time_all = time.time() - start_time\n",
    "        valid_loss, valid_acc = test_model(valid_loader, criterion, model, device)  # 测试模型\n",
    "        print('- Epoch: %d - Train_loss: %.5f - Train_acc: %.5f - Val_loss: %.5f - Val_acc: %5f - T_Time: %.3f 当前学习率：%f'\n",
    "              % (epoch, train_loss, train_acc, valid_loss, valid_acc, time_all, optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            # torch.save(model.state_dict(), '/content/drive/MyDrive/Colab/深度学习/心跳信号预测/best_model.pt')  # 保存最优模型\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    end = time.time()\n",
    "    run = end - start\n",
    "    print('[2]train： %.5f sec' % run)\n",
    "\n",
    "    print('******开始预测数据******')\n",
    "\n",
    "    model = CNN()\n",
    "    model.load_state_dict(torch.load('/content/drive/MyDrive/Colab/DeepLearning/心跳信号预测/best_model.pt'))  # 加载模型参数\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    predict_ali_testset(batch_size, model, device)  # 生成预测文件\n",
    "    print('[3] prediction done')\n",
    "    print('the end')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YrCwE9COeBJ6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1659168072899,
     "user_tz": -480,
     "elapsed": 5300,
     "user": {
      "displayName": "烨天杨",
      "userId": "13030508616284824368"
     }
    },
    "outputId": "26cc0d56-6419-4578-d2e3-b9d9438e1a81"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:1\n",
      "******开始预测数据******\n",
      "预测文件写入完成\n",
      "[3] prediction done\n",
      "the end\n"
     ]
    }
   ]
  }
 ]
}