{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d7dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # 这里，我们使用一个11 * 11的更大窗口了捕捉对象\n",
    "    # 同时 步幅为4 以减少输出的高度和宽度\n",
    "    # 另外 输出通道的数目远大于LeNet\n",
    "    nn.Conv2d(1, 96, kernel_size = 11,stride=4, padding = 1),nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),nn.ReLU(),\n",
    "    nn.Conv2d(96,256,kernel_size=5,padding=2),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "    # 使用三个连续的卷积层和较小的卷积窗口。\n",
    "    # 处理最后的卷积层 输出通道的数量进一步增加。\n",
    "    # 在前两个卷积层之后 汇聚层不用于减少输入的高度和宽度\n",
    "    nn.Conv2d(256,384,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.Conv2d(384,384,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.Conv2d(384,256,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "    nn.Flatten(),\n",
    "    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "    nn.Linear(6400,4096),nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096,4096),nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    # 最后是输出层 由于这里使用Fashion-MNIST 所以类别数为10,而非论文中的1000\n",
    "    nn.Linear(4096,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a705e",
   "metadata": {},
   "source": [
    "我们构造一个高度和宽度都为224的单通道数据，来观察每一层输出的形状。 它与 图7.1.2中的AlexNet架构相匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f039cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n",
      "ReLU output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "ReLU output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 256, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n",
      "Flatten output shape:\t torch.Size([1, 6400])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 1, 224, 224)\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9656a445",
   "metadata": {},
   "source": [
    "### 7.1.3. 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78eaea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5e89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f974c60",
   "metadata": {},
   "source": [
    "### 7.1.5. 小结\n",
    "- AlexNet的架构与LeNet相似，但使用了更多的卷积层和更多的参数来拟合大规模的ImageNet数据集。\n",
    "\n",
    "- 今天，AlexNet已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步。\n",
    "\n",
    "- 尽管AlexNet的代码只比LeNet多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具。\n",
    "\n",
    "- Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca2a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
